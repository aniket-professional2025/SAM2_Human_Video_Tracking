# Import required libraries
import cv2
import numpy as np
from ultralytics import YOLO, SAM
import os


# Define a function to use YOLO pose detection, SAM model on the input video and save the output as a file
def keypoints_draw_video_person_segmentation(input_video_path, output_video_path):

    print("Function for Segmenting Person/Human using Keypoints (Draw) from input video starts here....")

    # Load the Pose detection model
    pose_model = YOLO(r"C:\Users\Webbies\Jupyter_Notebooks\SAM2_VideoTracking\yolo11n-pose.pt")
    print("Successfully Loaded YOLO-pose model")

    # Load the SAM2 model
    sam_model = SAM(r"C:\Users\Webbies\Jupyter_Notebooks\SAM2_VideoTracking\sam2.1_b.pt")
    print("Successfully Loaded SAM2 model")

    # Load the video
    cap = cv2.VideoCapture(input_video_path)
    print("The video is loaded properly")

    # Get the video properties
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    print("The Video properties are fetched successfully")

    # Setting the output object
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))
    print("The output is set")

    # Processing Each Frame of the Video
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        print(f"Processing frame {frame_count}...")

        # Convert BGR frame to RGB (YOLO and SAM often prefer RGB, though YOLO can handle BGR)
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        annotated_frame = frame.copy() # Start with a copy of the original frame for annotations

        # 1. Perform YOLO-Pose inference
        # The 'stream=True' argument allows for generator-like results, useful for large videos 'conf=0.5' sets the confidence threshold for detections
        pose_results = pose_model(image_rgb, stream=False, conf=0.5, verbose=False)

        # Prepare bounding boxes for SAM
        person_bboxes = []
        for r in pose_results:
            # Iterate through each detected object in the current frame's results
            for box in r.boxes:
                # Check if the detected object is a 'person' (class ID 0 in COCO dataset)
                if int(box.cls[0]) == 0: # Assuming class 0 is 'person' for YOLOv8 pre-trained models
                    # Extract bounding box coordinates (xyxy format: x1, y1, x2, y2)
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    person_bboxes.append([x1, y1, x2, y2])

                    # Draw bounding box (optional, for visualization)
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 255), 2) # Cyan box

            # Draw keypoints if available
            if r.keypoints is not None and r.keypoints.xy is not None:
                for kpts in r.keypoints.xy: # Iterate through keypoints for each detected person
                    for kp in kpts: # Iterate through individual keypoints
                        x, y = int(kp[0]), int(kp[1])
                        if x > 0 and y > 0: # Check for valid keypoint coordinates
                            cv2.circle(annotated_frame, (x, y), 3, (0, 0, 255), -1) # Red circle for keypoints

        # 2. Perform SAM segmentation using the detected bounding boxes
        # Ultralytics SAM model takes the image and a list of bounding boxes as input for segmentation.
        if person_bboxes:
            # Pass the original image and the list of person bounding boxes to SAM
            sam_results = sam_model(image_rgb, bboxes=np.array(person_bboxes), verbose=False)

            for r in sam_results:
                if r.masks is not None:
                    # Iterate through each mask generated by SAM
                    for mask_tensor in r.masks.data:
                        # Convert the mask tensor to a numpy array and resize to original frame dimensions
                        mask = mask_tensor.cpu().numpy().astype(np.uint8) * 255
                        mask = cv2.resize(mask, (frame_width, frame_height), interpolation=cv2.INTER_NEAREST)

                        # Create a 3-channel mask for overlay
                        mask_3_channel = cv2.merge([mask, mask, mask])

                        # Create a colored overlay for the mask (e.g., green)
                        color_mask = np.zeros_like(annotated_frame, dtype=np.uint8)
                        color_mask[mask == 255] = [0, 255, 0] # Green color for the segmented area

                        # Blend the mask with the original frame
                        alpha = 0.5  # Transparency factor
                        annotated_frame = cv2.addWeighted(annotated_frame, 1, color_mask, alpha, 0)

        out.write(annotated_frame)

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    
    print("The process of segmenting persons/humans using Keypoints (and draw) finished successfully")


