# Import required libraries
import cv2
import numpy as np
from ultralytics import YOLO, SAM
import os

# Define a function to use YOLO pose detection, SAM model on the input video and save the output as a file
def person_diff_segmentation_mask(input_video_path, output_video_path):

    print("Function For Segmenting Person/Human using Keypoints (no Draw) and Different colored segmentation masks starts from here....")

    # Load the YOLO pose model
    pose_model = YOLO(r"C:\Users\Webbies\Jupyter_Notebooks\SAM2_VideoTracking\yolo11n-pose.pt")
    print("Successfully Loaded YOLO-pose model")

    # Load the SAM2 model
    sam_model = SAM(r"C:\Users\Webbies\Jupyter_Notebooks\SAM2_VideoTracking\sam2.1_b.pt")
    print("Successfully Loaded SAM2 model")

    # Load the video
    cap = cv2.VideoCapture(input_video_path)
    print("The video is loaded properly")

    # Get the video properties
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4
    print("The Video properties are fetched successfully")

    # Setting the output object
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))
    print("The output is set")

    # Define a list of distinct colors for segmentation masks (BGR format)
    # You can add more colors if you expect more than 5 persons simultaneously
    segmentation_colors = [
        (0, 255, 0),    # Green
        (255, 0, 0),    # Blue
        (0, 0, 255),    # Red
        (0, 255, 255),  # Yellow
        (255, 0, 255),  # Magenta
        (255, 255, 0),  # Cyan
        (128, 0, 128),  # Purple
        (0, 165, 255),  # Orange
        (0, 128, 0),    # Dark Green
    ]

    # Processing Each Frame of the Video
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        print(f"Processing frame {frame_count}...")

        # Convert BGR frame to RGB (YOLO and SAM often prefer RGB, though YOLO can handle BGR)
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        annotated_frame = frame.copy() # Start with a copy of the original frame for annotations

        # 1. Perform YOLO-Pose inference The 'stream=True' argument allows for generator-like results, useful for large videos
        # 'conf=0.5' sets the confidence threshold for detections
        pose_results = pose_model(image_rgb, stream=False, conf=0.5, verbose=False)

        # Prepare bounding boxes for SAM
        person_bboxes = []
        for r in pose_results:
            # Iterate through each detected object in the current frame's results
            for box in r.boxes:
                # Check if the detected object is a 'person' (class ID 0 in COCO dataset)
                if int(box.cls[0]) == 0: # Assuming class 0 is 'person' for YOLOv8 pre-trained models
                    # Extract bounding box coordinates (xyxy format: x1, y1, x2, y2)
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    person_bboxes.append([x1, y1, x2, y2])

                    # Draw bounding box (optional, for visualization)
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 255), 2) # Cyan box

            # Keypoints are used by YOLO-Pose internally but are not drawn on the annotated_frame.
            # (The block for drawing keypoints is intentionally omitted as per previous request.)

        # 2. Perform SAM segmentation using the detected bounding boxes
        # Ultralytics SAM model takes the image and a list of bounding boxes as input for segmentation.
        if person_bboxes:
            # Pass the original image and the list of person bounding boxes to SAM
            sam_results = sam_model(image_rgb, bboxes=np.array(person_bboxes), verbose=False)

            # Iterate through the SAM results and apply different colors
            mask_idx = 0
            for r in sam_results:
                if r.masks is not None:
                    # Iterate through each mask generated by SAM for this result object
                    # (In most cases with bboxes as input, r.masks.data will have one mask per bbox)
                    for mask_tensor in r.masks.data:
                        # Get the current color from our list, cycling if needed
                        current_color = segmentation_colors[mask_idx % len(segmentation_colors)]

                        # Convert the mask tensor to a numpy array and resize to original frame dimensions
                        mask = mask_tensor.cpu().numpy().astype(np.uint8) * 255
                        mask = cv2.resize(mask, (frame_width, frame_height), interpolation=cv2.INTER_NEAREST)

                        # Create a colored overlay for the mask using the current_color
                        color_mask = np.zeros_like(annotated_frame, dtype=np.uint8)
                        color_mask[mask == 255] = current_color # Apply the specific color

                        # Blend the mask with the original frame
                        alpha = 0.5  # Transparency factor
                        annotated_frame = cv2.addWeighted(annotated_frame, 1, color_mask, alpha, 0)

                        mask_idx += 1 # Increment mask index for the next person/mask

        out.write(annotated_frame)

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    
    print("The process of segmenting persons/humans using keypoints (without draw) and with different colored segmentation masks finished successfully")